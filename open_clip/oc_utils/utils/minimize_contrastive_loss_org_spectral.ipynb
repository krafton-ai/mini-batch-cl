{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from random import randint\n",
        "import math\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = 'cuda:0'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # making sure GPU runs are deterministic even if they are slower\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(\"Seeded everything: {}\".format(seed))\n",
        "\n",
        "def full_batch_loss(u, v):\n",
        "    n = u.shape[0]\n",
        "    logits = torch.exp(u @ v.T)\n",
        "    loss = -torch.log(logits/torch.sum(logits, dim=1)).diagonal(dim1=0).sum()\n",
        "    return loss/n\n",
        "\n",
        "def clip_batch_loss(u, v):\n",
        "    return full_batch_loss(u, v) + full_batch_loss(v, u)\n",
        "\n",
        "\n",
        "def mini_batch_loss(u, v, batch_idxs=None, B=2):\n",
        "    loss = 0\n",
        "    if batch_idxs == None:\n",
        "        # find all possible batches of size B\n",
        "        batch_idxs = list(itertools.combinations([i for i in range(u.shape[0])], B))\n",
        "    n = len(batch_idxs)\n",
        "    # print(\"len(batch_idxs)\", n, len(batch_idxs[0]))\n",
        "    loss_list = []\n",
        "    for batch_idx in batch_idxs:\n",
        "        u_batch = u[list(batch_idx)]\n",
        "        v_batch = v[list(batch_idx)]\n",
        "        loss += clip_batch_loss(u_batch, v_batch)\n",
        "        loss_list.append(clip_batch_loss(u_batch, v_batch))\n",
        "    return loss/n, loss_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output_dir: output_uni/2023_03_31-12_11_35_hst_bi_N200_d2_lr0.5_s200x5_all_Bs[20]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# import argparse\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--N\", type=int, default=20)\n",
        "# parser.add_argument(\"--d\", type=int, default=32)\n",
        "# parser.add_argument(\"--lr_full\", type=float, default=0.5)\n",
        "# # parser.add_argument(\"--lr_scaling\", default=False, action='store_true')\n",
        "# parser.add_argument(\"--num_steps\", type=int, default=10000)\n",
        "# parser.add_argument(\"--logging_step_ratio\", type=float, default=0.1)\n",
        "# # parser.add_argument(\"--gradient_accumulation\", default=False, action='store_true')\n",
        "# parser.add_argument(\"--batch_size_list\", nargs='+', type=int, default=[2])\n",
        "# parser.add_argument(\"--wandb_notes\", default=\"\", type=str, help=\"additional wandb logging note\")\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# print(\"N\", args.N)\n",
        "# print(\"d\", args.d)\n",
        "# print(\"lr_full\", args.lr_full)\n",
        "# # print(\"lr_scaling\", args.lr_scaling)\n",
        "# print(\"num_steps\", args.num_steps)\n",
        "# print(\"logging_step_ratio\", args.logging_step_ratio)\n",
        "# # print(\"gradient_accumulation\", args.gradient_accumulation)\n",
        "# print(\"batch_size_list\", args.batch_size_list)\n",
        "\n",
        "N = 200 #args.N\n",
        "d = 2 #args.d\n",
        "n_cluster = 10\n",
        "lr_full = 0.5 #args.lr_full\n",
        "batch_size_list = [20] #args.batch_size_list\n",
        "NUM_STEPS = 200 #args.num_steps\n",
        "num_steps_factor = 5 if N > 12 else 1\n",
        "# logging_step = int(NUM_STEPS * args.logging_step_ratio)\n",
        "logging_step = 1\n",
        "time_tag = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "which_modal = 'bi'\n",
        "exp_tag = f\"{time_tag}_hst_{which_modal}_N{N}_d{d}_lr{lr_full}_s{NUM_STEPS}x{num_steps_factor}_all_Bs{batch_size_list}\"\n",
        "output_dir = f\"output_uni/{exp_tag}\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(\"output_dir:\", output_dir)\n",
        "\n",
        "\n",
        "def get_embeddings(N, d):\n",
        "    u = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    v = u.clone().detach() + torch.randn((N, d), requires_grad=False, device=device)\n",
        "    # v = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    with torch.no_grad():\n",
        "        u.data = F.normalize(u.data, p=2.0, dim=1)\n",
        "        v.data = F.normalize(v.data, p=2.0, dim=1)\n",
        "    return (u, v)\n",
        "\n",
        "def get_unimodal_embeddings(N, d):\n",
        "    u = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    v = u.clone().detach() + torch.randn((N, d), requires_grad=False, device=device)\n",
        "    # v = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    with torch.no_grad():\n",
        "        u.data = F.normalize(u.data, p=2.0, dim=1)\n",
        "        v.data = F.normalize(v.data, p=2.0, dim=1)\n",
        "    return (u, v)\n",
        "\n",
        "def get_bimodal_embeddings(N, d):\n",
        "    u = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    v = torch.randn((N, d), requires_grad=True, device=device)\n",
        "    with torch.no_grad():\n",
        "        u.data = F.normalize(u.data, p=2.0, dim=1)\n",
        "        v.data = F.normalize(v.data, p=2.0, dim=1)\n",
        "    return (u, v)\n",
        "\n",
        "def plot_embeddings(u, v, d, is_proj=True, filename=None):\n",
        "    circle = plt.Circle((0, 0), 1, color='black', fill=False)\n",
        "    if is_proj:\n",
        "        # project down to 2d to visualize\n",
        "        linear_projection = torch.randn(d, 2)\n",
        "        proj_u = F.normalize(u.to('cpu')@linear_projection.detach(), p=2.0, dim=1)\n",
        "        proj_v = F.normalize(v.to('cpu')@linear_projection.detach(), p=2.0, dim=1)\n",
        "    else:\n",
        "        proj_u = u.to('cpu')\n",
        "        proj_v = v.to('cpu')\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.add_patch(circle)\n",
        "    ax.scatter(proj_u[:, 0].detach().numpy(), proj_u[:, 1].detach().numpy(), color='blue', label='u', marker=\"+\", s=150)\n",
        "    for i in range(proj_u.shape[0]):\n",
        "        ax.text(proj_u[i, 0], proj_u[i, 1], f'u_{i}')\n",
        "    ax.scatter(proj_v[:, 0].detach().numpy(), proj_v[:, 1].detach().numpy(), color='red', label='v')\n",
        "    for i in range(proj_v.shape[0]):\n",
        "        ax.text(proj_v[i, 0], proj_v[i, 1], f'v_{i}')\n",
        "    ax.legend(loc='best')\n",
        "    plt.grid()\n",
        "    #plt.show()\n",
        "    if filename is not None:\n",
        "        #import ipdb; ipdb.set_trace()\n",
        "        plt.savefig(f'{filename}.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
        "    plt.close('all')\n",
        "    return proj_u, proj_v\n",
        "\n",
        "\n",
        "def heatmap(data, row_labels, col_labels, ax=None,\n",
        "            cbar_kw=None, cbarlabel=\"\", plot_cbar=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a heatmap from a numpy array and two lists of labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        A 2D numpy array of shape (M, N).\n",
        "    row_labels\n",
        "        A list or array of length M with the labels for the rows.\n",
        "    col_labels\n",
        "        A list or array of length N with the labels for the columns.\n",
        "    ax\n",
        "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
        "        not provided, use current axes or create a new one.  Optional.\n",
        "    cbar_kw\n",
        "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
        "    cbarlabel\n",
        "        The label for the colorbar.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to `imshow`.\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    if cbar_kw is None:\n",
        "        cbar_kw = {}\n",
        "\n",
        "    # Plot the heatmap\n",
        "    im = ax.imshow(data, **kwargs)\n",
        "\n",
        "    # Create colorbar\n",
        "    cbar = None\n",
        "    if plot_cbar:\n",
        "        cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
        "        cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries.\n",
        "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
        "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
        "\n",
        "    # Let the horizontal axes labeling appear on top.\n",
        "    ax.tick_params(top=True, bottom=False,\n",
        "                   labeltop=True, labelbottom=False)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Turn spines off and create white grid.\n",
        "    ax.spines[:].set_visible(False)\n",
        "\n",
        "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
        "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "    return im, cbar\n",
        "\n",
        "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
        "                     textcolors=(\"black\", \"white\"),\n",
        "                     threshold=None, **textkw):\n",
        "    \"\"\"\n",
        "    A function to annotate a heatmap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im\n",
        "        The AxesImage to be labeled.\n",
        "    data\n",
        "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
        "    valfmt\n",
        "        The format of the annotations inside the heatmap.  This should either\n",
        "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
        "        `matplotlib.ticker.Formatter`.  Optional.\n",
        "    textcolors\n",
        "        A pair of colors.  The first is used for values below a threshold,\n",
        "        the second for those above.  Optional.\n",
        "    threshold\n",
        "        Value in data units according to which the colors from textcolors are\n",
        "        applied.  If None (the default) uses the middle of the colormap as\n",
        "        separation.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to each call to `text` used to create\n",
        "        the text labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(data, (list, np.ndarray)):\n",
        "        data = im.get_array()\n",
        "\n",
        "    # Normalize the threshold to the images color range.\n",
        "    if threshold is not None:\n",
        "        threshold = im.norm(threshold)\n",
        "    else:\n",
        "        threshold = im.norm(data.max())/2.\n",
        "\n",
        "    # Set default alignment to center, but allow it to be\n",
        "    # overwritten by textkw.\n",
        "    kw = dict(horizontalalignment=\"center\",\n",
        "              verticalalignment=\"center\")\n",
        "    kw.update(textkw)\n",
        "\n",
        "    # Get the formatter in case a string is supplied\n",
        "    if isinstance(valfmt, str):\n",
        "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
        "\n",
        "    # Loop over the data and create a `Text` for each \"pixel\".\n",
        "    # Change the text's color depending on the data.\n",
        "    texts = []\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
        "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\n",
        "def plot_heatmap(z, filename=None, plot_cbar=True):\n",
        "    N = z.shape[0]\n",
        "    fig, ax = plt.subplots()\n",
        "    im, cbar = heatmap(z, np.arange(N), np.arange(N), ax=ax, plot_cbar=plot_cbar, cmap=\"YlGn\", vmin=-1.0, vmax=1.0)\n",
        "    texts = annotate_heatmap(im, valfmt=\"\")#\"{x:.4f}\")\n",
        "    fig.tight_layout()\n",
        "    if filename is not None:\n",
        "        plt.savefig(f'{filename}.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
        "        plt.savefig(f'{filename}.pdf', format='pdf', dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
        "    plt.close('all')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# if wandb:\n",
        "#     print(f\"init wandb logging for full_batch training ...\")\n",
        "#     exp_name = '-'.join([\n",
        "#         f\"full_batch\",\n",
        "#     ])\n",
        "\n",
        "#     wandb.init(\n",
        "#         entity=\"krafton_clap\",\n",
        "#         project=\"simulations\",\n",
        "#         group=f\"{exp_tag}\",\n",
        "#         name=exp_name,\n",
        "#         notes='',\n",
        "#         # config=vars(args)\n",
        "#     )\n",
        "\n",
        "# ## First minimize full_batch loss\n",
        "# u1, v1 = get_embeddings(N, d)\n",
        "# param_list = [u1, v1]\n",
        "\n",
        "# loss_dict = {}\n",
        "# optimizer = torch.optim.SGD(param_list, lr=lr_full)\n",
        "# for step in range(NUM_STEPS*num_steps_factor):\n",
        "#     optimizer.zero_grad()\n",
        "#     loss = clip_batch_loss(u1, v1)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     with torch.no_grad():\n",
        "#         u1.data = F.normalize(u1.data, p=2.0, dim = 1)\n",
        "#         v1.data = F.normalize(v1.data, p=2.0, dim = 1)\n",
        "#     if step %logging_step == 0 or step == NUM_STEPS-1:\n",
        "#         # print(\"Step={} | Loss={} | Grad Norm={}\".format(step, loss, torch.norm(u1.grad.data)))\n",
        "#         torch.save(u1, f\"{output_dir}/u1_full_batch_{step}.pt\")\n",
        "#         torch.save(v1, f\"{output_dir}/v1_full_batch_{step}.pt\")\n",
        "#         loss_dict[step] = loss.item()\n",
        "#         if wandb:\n",
        "#             assert wandb is not None, 'Please install wandb.'\n",
        "#             wandb.log({\n",
        "#                 'step': step,\n",
        "#                 'loss': loss_dict[step],\n",
        "#                 'true_loss': loss_dict[step],\n",
        "#             })\n",
        "# with open(f'{output_dir}/loss_full_batch.json', 'w') as f:\n",
        "#     f.write(json.dumps(loss_dict, indent=4))\n",
        "\n",
        "\n",
        "# ## Check if it is ETF (It works!)\n",
        "# # first see if u = v\n",
        "# # print(\"||u-v|| = {}\".format(torch.norm(u1-v1)))\n",
        "# # now see if the inner products are equal\n",
        "# # z1 = (u1 @ v1.T).detach().cpu()\n",
        "# # # print(\"u^T v={}\".format(z1))\n",
        "# # torch.save(z1, f\"{output_dir}/z1_full_batch_{step}.pt\")\n",
        "\n",
        "# # u1_proj, v1_proj = plot_embeddings(u1, v1, d, filename=f'{output_dir}/plot_embeddings_full_batch')\n",
        "# # plot_heatmap(z1, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z1_full_batch_w_cbar\"), plot_cbar=True)\n",
        "# # plot_heatmap(z1, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z1_full_batch_wo_cbar\"), plot_cbar=False)\n",
        "\n",
        "# # if wandb:\n",
        "# #     z1_w_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z1_full_batch_w_cbar\" + \".png\"))\n",
        "# #     z1_wo_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z1_full_batch_wo_cbar\" + \".png\"))\n",
        "# #     wandb.log({\"heatmap_w_cbar\": [z1_w_cbar]})\n",
        "# #     wandb.log({\"heatmap_wo_cbar\": [z1_wo_cbar]})\n",
        "# #     wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ## Now minimize mini-batch loss over all (NcB) batches\n",
        "# for B in batch_size_list:\n",
        "#     set_seed(42)\n",
        "\n",
        "#     if wandb:\n",
        "#         print(f\"init wandb logging for all (NcB) batches training B{B}...\")\n",
        "#         exp_name = '-'.join([\n",
        "#             f\"NcB_B{B}\",\n",
        "#         ])\n",
        "\n",
        "#         wandb.init(\n",
        "#             entity=\"krafton_clap\",\n",
        "#             project=\"simulations\",\n",
        "#             group=f\"{exp_tag}\",\n",
        "#             name=exp_name,\n",
        "#             #notes=args.wandb_notes,\n",
        "#             #config=vars(args)\n",
        "#         )\n",
        "\n",
        "#     u2, v2 = get_embeddings(N, d)\n",
        "#     param_list = [u2, v2]\n",
        "\n",
        "#     loss_dict, true_loss_dict = {}, {}\n",
        "#     optimizer = torch.optim.SGD(param_list, lr=lr_full)\n",
        "#     for step in tqdm(range(NUM_STEPS*num_steps_factor), desc=f\"[batch_selection:'All NcB' | B:{B}] \"):\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = mini_batch_loss(u2, v2, B=B)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         with torch.no_grad():\n",
        "#             u2.data = F.normalize(u2.data, p=2.0, dim = 1)\n",
        "#             v2.data = F.normalize(v2.data, p=2.0, dim = 1)\n",
        "#         if step %logging_step == 0 or step == NUM_STEPS-1:\n",
        "#             # print(\"B={} | Step={} | Loss={} | Grad Norm={}\".format(B, step, loss, torch.norm(u2.grad.data)))\n",
        "#             torch.save(u2, f\"{output_dir}/u2_NcB_mini_batch_B{B}_{step}.pt\")\n",
        "#             torch.save(v2, f\"{output_dir}/v2_NcB_mini_batch_B{B}_{step}.pt\")\n",
        "#             loss_dict[step], true_loss_dict[step] = loss.item(), clip_batch_loss(u2, v2).detach().item()\n",
        "#             if wandb:\n",
        "#                 assert wandb is not None, 'Please install wandb.'\n",
        "#                 wandb.log({\n",
        "#                     'step': step,\n",
        "#                     'loss': loss_dict[step],\n",
        "#                     'true_loss': true_loss_dict[step],\n",
        "#                 })\n",
        "#     with open(f'{output_dir}/loss_NcB_mini_batch_B{B}.json', 'w') as f:\n",
        "#         f.write(json.dumps(loss_dict, indent=4))\n",
        "#     with open(f'{output_dir}/true_loss_NcB_mini_batch_B{B}.json', 'w') as f:\n",
        "#         f.write(json.dumps(true_loss_dict, indent=4))\n",
        "\n",
        "#     ## Check if it is ETF (It works!)\n",
        "#     # first see if u = v\n",
        "#     # print(\"||u-v|| = {}\".format(torch.norm(u2-v2)))\n",
        "#     # now see if the inner products are equal\n",
        "#     # z2 = (u2 @ v2.T).detach().cpu()\n",
        "#     # # print(\"u^T v={}\".format(z2))\n",
        "#     # torch.save(z2, f\"{output_dir}/z2_NcB_mini_batch_B{B}_{step}.pt\")\n",
        "\n",
        "#     # u2_proj, v2_proj = plot_embeddings(u2, v2, d, filename=f'{output_dir}/plot_embeddings_NcB_mini_batch_B{B}')\n",
        "#     # plot_heatmap(z2, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z2_NcB_mini_batch_B{B}_w_cbar\"), plot_cbar=True)\n",
        "#     # plot_heatmap(z2, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z2_NcB_mini_batch_B{B}_wo_cbar\"), plot_cbar=False)\n",
        "\n",
        "#     # if wandb:\n",
        "#     #     z2_w_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z2_NcB_mini_batch_B{B}_w_cbar\" + \".png\"))\n",
        "#     #     z2_wo_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z2_NcB_mini_batch_B{B}_wo_cbar\" + \".png\"))\n",
        "#     #     wandb.log({\"heatmap_w_cbar\": [z2_w_cbar]})\n",
        "#     #     wandb.log({\"heatmap_wo_cbar\": [z2_wo_cbar]})\n",
        "#     #     wandb.finish()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from batch_utils import create_inverse_greedy_batches_with_K, create_balance_greedy_batches, create_greedy_batches\n",
        "def get_random_batch_idxs(N, B=2):\n",
        "    batch_idxs = np.arange(N)\n",
        "    np.random.shuffle(batch_idxs)\n",
        "    if (N % B) == 0:\n",
        "        batch_idxs = batch_idxs.reshape(-1, B).tolist()\n",
        "    elif (N // B) == 1: # allow overlap between batches\n",
        "        batch_idxs = [batch_idxs[:B].tolist(), batch_idxs[-B:].tolist()]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return batch_idxs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def my_affinity(U, V):\n",
        "    # Compute the pairwise Euclidean distances between data points\n",
        "    Z = U@np.transpose(V)  # UV\n",
        "    Z_T = np.transpose(Z)  # VU\n",
        "\n",
        "    d = np.diag(Z)  ## UU\n",
        "    Z_sub = np.transpose(d*np.ones((len(d), len(d))))\n",
        "\n",
        "    affinity = Z + Z_T - 2*Z_sub\n",
        "    affinity += np.transpose(affinity)\n",
        "\n",
        "    return affinity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sc_batches(u3, v3, n_cluster):\n",
        "\n",
        "    sc = SpectralClustering(n_cluster, affinity='precomputed', eigen_solver='arpack')\n",
        "    # affinity = my_affinity_v2(u3.cpu().detach().numpy(), v3.cpu().detach().numpy())\n",
        "    affinity = my_affinity(u3, v3)\n",
        "    y_pred = sc.fit_predict(np.exp(affinity))\n",
        "\n",
        "    batch_idxs = []\n",
        "    for t in set(y_pred):\n",
        "        batch_idx = np.where(np.array(y_pred) == t)[0].tolist()\n",
        "        if len(batch_idx) > 1:\n",
        "            batch_idxs.append(batch_idx)\n",
        "    #print(f\"batch_idxs : {batch_idxs}\")\n",
        "\n",
        "    return batch_idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for f B20...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:uau32w98) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>loss_std</td><td>██▇▇▆▆▅▅▄▃▄▃▂▂▂▂▂▂▂▂▂▁▃▂▂▂▂▁▂▂▂▃▁▁▂▁▂▃▂▂</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>5.79154</td></tr><tr><td>loss_std</td><td>0.31823</td></tr><tr><td>step</td><td>953</td></tr><tr><td>true_loss</td><td>9.07543</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">g_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/uau32w98' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/uau32w98</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_120654-uau32w98/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uau32w98). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_121136-7zodsjns</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/7zodsjns' target=\"_blank\">f_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/7zodsjns' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/7zodsjns</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'f' | B:20] : 100%|██████████| 1000/1000 [00:13<00:00, 73.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for s B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:7zodsjns) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>███▇█████▇▇▆▆▅▅▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>4.46953</td></tr><tr><td>avg_batch_loss_std</td><td>0.00673</td></tr><tr><td>elapse_time</td><td>13</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.06901</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">f_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/7zodsjns' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/7zodsjns</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_121136-7zodsjns/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:7zodsjns). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_121201-hplq4w31</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/hplq4w31' target=\"_blank\">s_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/hplq4w31' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/hplq4w31</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'s' | B:20] : 100%|██████████| 1000/1000 [00:13<00:00, 74.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for g B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:hplq4w31) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>█▆▅▃▃▅▆▃▄▃▄▃▂▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▂▁▂▁▁▂▂▁▂▁▂▁</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>4.5451</td></tr><tr><td>avg_batch_loss_std</td><td>0.04363</td></tr><tr><td>elapse_time</td><td>13</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.06891</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">s_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/hplq4w31' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/hplq4w31</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_121201-hplq4w31/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:hplq4w31). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_121225-1zl9rq6s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/1zl9rq6s' target=\"_blank\">g_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/1zl9rq6s' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/1zl9rq6s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'g' | B:20] : 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for bg B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1zl9rq6s) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>██▇▇▆▆▅▅▄▃▃▂▂▂▃▁▁▂▂▂▁▃▂▂▂▂▁▃▂▂▃▂▁▁▂▃▂▂▂▂</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▇▆▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>5.80501</td></tr><tr><td>avg_batch_loss_std</td><td>0.17755</td></tr><tr><td>elapse_time</td><td>169</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.0743</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">g_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/1zl9rq6s' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/1zl9rq6s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_121225-1zl9rq6s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1zl9rq6s). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_121525-uifh3vne</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/uifh3vne' target=\"_blank\">bg_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/uifh3vne' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/uifh3vne</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'bg' | B:20] : 100%|██████████| 1000/1000 [02:28<00:00,  6.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for ig B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:uifh3vne) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▇▆▅▅▄▄▅▄▄▄▃▃▃▃▃▂▂▂▃▂▁▂▂▂▂▃▂▃▂▂▁▁▂▂▂▂▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>▁▃▂▁▄▃▃▆▁▅▂▄▄▄▂▃▃▄▇▅▇▇▆██▃▆▂▃▂▅▅█▆▅▆▃▄▇▆</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>5.31176</td></tr><tr><td>avg_batch_loss_std</td><td>0.32633</td></tr><tr><td>elapse_time</td><td>148</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.09249</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bg_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/uifh3vne' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/uifh3vne</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_121525-uifh3vne/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uifh3vne). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_121804-2igll1cx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/2igll1cx' target=\"_blank\">ig_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/2igll1cx' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/2igll1cx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'ig' | B:20] : 100%|██████████| 1000/1000 [03:44<00:00,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for osgd B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:2igll1cx) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▇▆▆▄▄▃▃▂▃▃▂▂▂▂▁▁▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>▆█▇▃▇▆▆▂▅▅▅▃▁▂▅▄▂▆▂▃▃▄▄▆▂▁▄▅▅▆▃▄▄▅▃▅▅▂▃▂</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>4.82343</td></tr><tr><td>avg_batch_loss_std</td><td>0.24816</td></tr><tr><td>elapse_time</td><td>224</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.06911</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ig_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/2igll1cx' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/2igll1cx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_121804-2igll1cx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2igll1cx). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_122159-6dq61gzg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/6dq61gzg' target=\"_blank\">osgd_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/6dq61gzg' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/6dq61gzg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'osgd' | B:20] : 100%|██████████| 1000/1000 [02:55<00:00,  5.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeded everything: 42\n",
            "init wandb logging for sc B20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:6dq61gzg) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_batch_loss_std</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>elapse_time</td><td>▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>true_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_batch_loss</td><td>5.93053</td></tr><tr><td>avg_batch_loss_std</td><td>0.24816</td></tr><tr><td>elapse_time</td><td>175</td></tr><tr><td>step</td><td>999</td></tr><tr><td>true_loss</td><td>9.12214</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">osgd_B20</strong> at: <a href='https://wandb.ai/krafton_clap/simulations/runs/6dq61gzg' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/6dq61gzg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230331_122159-6dq61gzg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:6dq61gzg). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/clap/projects/jglee/gitlab_open_clip/open_clip/src/utils/wandb/run-20230331_122506-ndkibuox</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/krafton_clap/simulations/runs/ndkibuox' target=\"_blank\">sc_B20</a></strong> to <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/krafton_clap/simulations' target=\"_blank\">https://wandb.ai/krafton_clap/simulations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/krafton_clap/simulations/runs/ndkibuox' target=\"_blank\">https://wandb.ai/krafton_clap/simulations/runs/ndkibuox</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[batch_selection:'sc' | B:20] : 100%|██████████| 1000/1000 [01:19<00:00, 12.64it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_selections = ['f', 's', 'g', 'bg', 'ig', 'osgd', 'sc']\n",
        "# batch_selections = ['sc']\n",
        "## Now minimize mini-batch loss over all specific batches\n",
        "for B in batch_size_list:\n",
        "\n",
        "    batch_idxs = get_random_batch_idxs(N, B)\n",
        "\n",
        "    for batch_selection in batch_selections:\n",
        "        set_seed(42)\n",
        "\n",
        "        if wandb:\n",
        "            print(f\"init wandb logging for {batch_selection} B{B}...\")\n",
        "            exp_name = '-'.join([\n",
        "                f\"{batch_selection}_B{B}\",\n",
        "            ])\n",
        "\n",
        "            wandb.init(\n",
        "                entity=\"krafton_clap\",\n",
        "                project=\"simulations\",\n",
        "                group=f\"{exp_tag}\",\n",
        "                name=exp_name,\n",
        "                notes='',\n",
        "                #config=vars(args)\n",
        "            )\n",
        "        \n",
        "        if which_modal == 'bi':\n",
        "            u3, v3 = get_bimodal_embeddings(N, d)\n",
        "        elif which_modal == 'uni':\n",
        "            u3, v3 = get_unimodal_embeddings(N, d)\n",
        "        param_list = [u3, v3]\n",
        "\n",
        "        loss_dict, true_loss_dict = {}, {}\n",
        "        optimizer = torch.optim.SGD(param_list, lr=lr_full) #lr=args.lr_full)\n",
        "        start = datetime.now()\n",
        "        for step in tqdm(range(NUM_STEPS*num_steps_factor), desc=f\"[batch_selection:'{batch_selection}' | B:{B}] \"):\n",
        "            if batch_selection == 'osgd':\n",
        "                total_loss = 0\n",
        "                for _ in range(len(batch_idxs)): # different meaning of epochs in here\n",
        "                    with torch.no_grad():\n",
        "                        batch_idx = create_greedy_batches(N, B, u3.detach(), v3.detach(), 1.0, device=device, D=d, max_B=B, max_n_batch=1)[0]\n",
        "                    optimizer.zero_grad()\n",
        "                    u_batch = u3[list(batch_idx)]\n",
        "                    v_batch = v3[list(batch_idx)]\n",
        "                    loss = clip_batch_loss(u_batch, v_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss\n",
        "                loss = total_loss / len(batch_idxs)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    if batch_selection == 'f':\n",
        "                        pass\n",
        "                    elif batch_selection == 's':\n",
        "                        batch_idxs = get_random_batch_idxs(N, B)\n",
        "                    elif batch_selection == 'g':\n",
        "                        batch_idxs = create_greedy_batches(N, B, u3.detach(), v3.detach(), 1.0, device=device, D=d)\n",
        "                    elif batch_selection == 'bg':\n",
        "                        batch_idxs = create_balance_greedy_batches(N, B, u3.detach(), v3.detach(), 1.0, device=device, D=d)\n",
        "                    elif batch_selection == 'ig':\n",
        "                        batch_idxs = create_inverse_greedy_batches_with_K(N, B, u3.detach(), v3.detach(), 1.0, device=device, D=d)\n",
        "                    elif batch_selection == 'sc':\n",
        "                        batch_idxs = create_sc_batches(u3, v3, n_cluster)\n",
        "                    else:\n",
        "                        raise NotImplementedError(f'{batch_selection} is not available for batching')\n",
        "                    \n",
        "                optimizer.zero_grad()\n",
        "                loss, loss_list = mini_batch_loss(u3, v3, batch_idxs=batch_idxs)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                u3.data = F.normalize(u3.data, p=2.0, dim = 1)\n",
        "                v3.data = F.normalize(v3.data, p=2.0, dim = 1)\n",
        "            if step %logging_step == 0 or step == NUM_STEPS-1:\n",
        "                # print(\"B={} | Step={} | Loss={} | Grad Norm={}\".format(B, step, loss, torch.norm(u3.grad.data)))\n",
        "                torch.save(u3, f\"{output_dir}/u3_{batch_selection}_mini_batch_B{B}_{step}.pt\")\n",
        "                torch.save(v3, f\"{output_dir}/v3_{batch_selection}_mini_batch_B{B}_{step}.pt\")\n",
        "                loss_dict[step], true_loss_dict[step] = loss.item(), clip_batch_loss(u3, v3).detach().item()\n",
        "                if wandb:\n",
        "                    assert wandb is not None, 'Please install wandb.'\n",
        "                    wandb.log({\n",
        "                        'step': step,\n",
        "                        'avg_batch_loss': loss_dict[step],\n",
        "                        #'avg_batch_loss': loss_dict[step],\n",
        "                        'avg_batch_loss_std': torch.std(torch.stack(loss_list)),\n",
        "                        'true_loss': true_loss_dict[step],\n",
        "                    })\n",
        "        end = datetime.now()\n",
        "        wandb.log({'elapse_time' : (end-start).seconds})\n",
        "        \n",
        "        with open(f'{output_dir}/loss_{batch_selection}_mini_batch_B{B}.json', 'w') as f:\n",
        "            f.write(json.dumps(loss_dict, indent=4))\n",
        "        with open(f'{output_dir}/true_loss_{batch_selection}_mini_batch_B{B}.json', 'w') as f:\n",
        "            f.write(json.dumps(true_loss_dict, indent=4))\n",
        "\n",
        "        # # first see if u = v\n",
        "        # # print(\"||u-v|| = {}\".format(torch.norm(u3-v3)))\n",
        "        # # now see if the inner products are equal\n",
        "        # z3 = (u3 @ v3.T).detach().cpu()\n",
        "        # # print(\"u^T v={}\".format(z3))\n",
        "        # torch.save(z3, f\"{output_dir}/z3_{batch_selection}_mini_batch_B{B}_{step}.pt\")\n",
        "\n",
        "        # u3_proj, v3_proj = plot_embeddings(u3, v3, d, filename=f'{output_dir}/plot_embeddings_{batch_selection}_mini_batch_B{B}')\n",
        "        # plot_heatmap(z3, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z3_fixed_mini_batch_B{B}_{batch_selection}_w_cbar\"), plot_cbar=True)\n",
        "        # plot_heatmap(z3, filename=os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z3_fixed_mini_batch_B{B}_{batch_selection}_wo_cbar\"), plot_cbar=False)\n",
        "\n",
        "        # if wandb:\n",
        "        #     z3_w_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z3_fixed_mini_batch_B{B}_{batch_selection}_w_cbar\" + \".png\"))\n",
        "        #     z3_wo_cbar = wandb.Image(os.path.join(output_dir, f\"N{N}_d{d}_lr{lr_full}_s{NUM_STEPS*num_steps_factor}_z3_fixed_mini_batch_B{B}_{batch_selection}_wo_cbar\" + \".png\"))\n",
        "        #     wandb.log({\"heatmap_w_cbar\": [z3_w_cbar]})\n",
        "        #     wandb.log({\"heatmap_wo_cbar\": [z3_wo_cbar]})\n",
        "        #     wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "end = datetime.now()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "end2 = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(end2 - end).seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
