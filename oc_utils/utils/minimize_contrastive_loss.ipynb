{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randint\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = 'cuda:0' # ['cpu', 'cuda:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # making sure GPU runs are deterministic even if they are slower\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Seeded everything: {}\".format(seed))\n",
    "\n",
    "\n",
    "def full_batch_loss(u, v):\n",
    "    n = u.shape[0]\n",
    "    logits = torch.exp(u @ v.T)\n",
    "    loss = -torch.log(logits/torch.sum(logits, dim=1)).diagonal(dim1=0).sum()\n",
    "    return loss/n\n",
    "\n",
    "def clip_batch_loss(u, v):\n",
    "    return full_batch_loss(u, v) + full_batch_loss(v, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ce0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_loss(u, v, batch_idxs=None, B=2):\n",
    "    loss = 0\n",
    "    if batch_idxs == None:\n",
    "        # find all possible batches of size B\n",
    "        batch_idxs = list(itertools.combinations([i for i in range(u.shape[0])], B))\n",
    "    n = len(batch_idxs)\n",
    "    for batch_idx in batch_idxs:\n",
    "        u_batch = u[list(batch_idx)]\n",
    "        v_batch = v[list(batch_idx)]\n",
    "        loss += clip_batch_loss(u_batch, v_batch)\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d640460",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "d = 32\n",
    "Bs = [2, 5, 10]\n",
    "dict_template = { # in the order of algorithms & in the order of B\n",
    "    \"u\": [],\n",
    "    \"v\": [],\n",
    "    \"metrics\": [],\n",
    "    \"true_loss\": [],\n",
    "    \"loss\": [],\n",
    "    \"norm\": [],\n",
    "}\n",
    "res = defaultdict(lambda:copy.deepcopy(dict_template))\n",
    "\n",
    "lr_full = 0.5\n",
    "lr_scaling = True\n",
    "NUM_STEPS = 1000\n",
    "logging_step = int(NUM_STEPS * 0.1)\n",
    "gradient_accumulation = True\n",
    "\n",
    "ga_tag = f\"_accum\" if gradient_accumulation else \"\"\n",
    "lrs_tag = f\"_lrs\" if lr_scaling else \"\"\n",
    "output_dir = f\"output/sim_plots_N{N}_d{d}_lr{lr_full}{ga_tag}{lrs_tag}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def get_embeddings(N, d):\n",
    "    u = torch.randn((N, d), requires_grad=True, device=device)\n",
    "    v = torch.randn((N, d), requires_grad=True, device=device)\n",
    "    with torch.no_grad():\n",
    "        u.data = F.normalize(u.data, p=2.0, dim=1)\n",
    "        v.data = F.normalize(v.data, p=2.0, dim=1)\n",
    "    return (u, v)\n",
    "\n",
    "def plot_embeddings(u, v, d):\n",
    "    # project down to 2d to visualize\n",
    "    linear_projection = torch.randn(d, 2)\n",
    "    proj_u = F.normalize(u.to('cpu')@linear_projection.detach(), p=2.0, dim=1)\n",
    "    proj_v = F.normalize(v.to('cpu')@linear_projection.detach(), p=2.0, dim=1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(proj_u[:, 0].detach().numpy(), proj_u[:, 1].detach().numpy(), color='blue', label='u', marker=\"+\", s=150)\n",
    "    ax.scatter(proj_v[:, 0].detach().numpy(), proj_v[:, 1].detach().numpy(), color='red', label='v')\n",
    "    ax.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return proj_u, proj_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mrr(indices):\n",
    "    mrr = []\n",
    "    for ii, inds in enumerate(indices):\n",
    "        mrr.append(1 / (inds.tolist().index(ii)+1))\n",
    "    return np.mean(mrr)\n",
    "\n",
    "def get_metrics(image_features, text_features, logit_scale):\n",
    "    metrics = {}\n",
    "    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()\n",
    "    logits_per_text = logits_per_image.t().detach().cpu()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "    ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        preds = torch.where(ranking == ground_truth)[1]\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        metrics[f\"{name}_mrr\"] = _get_mrr(ranking)\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d7526",
   "metadata": {},
   "source": [
    "## First minimize full_batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d49fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "u1, v1 = get_embeddings(N, d)\n",
    "param_list = [u1, v1]\n",
    "\n",
    "_, _ = plot_embeddings(u1[:30], v1[:30], d)\n",
    "\n",
    "loss_full, norm_full, true_loss_full, metrics_full = {}, {}, {}, {}\n",
    "optimizer = torch.optim.SGD(param_list, lr=lr_full)\n",
    "for step in tqdm(range(NUM_STEPS), desc=\"[Full Batch]\"):\n",
    "    optimizer.zero_grad()\n",
    "    loss = clip_batch_loss(u1, v1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        u1.data = F.normalize(u1.data, p=2.0, dim=1)\n",
    "        v1.data = F.normalize(v1.data, p=2.0, dim=1)\n",
    "    if step %logging_step == 0 or step == NUM_STEPS-1:\n",
    "        norm = torch.norm(u1.grad.data)\n",
    "        # print(\"Step={} | Loss={} | Grad Norm={}\".format(step, loss, norm))\n",
    "        loss_full[step], norm_full[step], true_loss_full[step] = loss.detach().item(), norm.detach().item(), loss.detach().item()\n",
    "        metrics_full[step] = get_metrics(u1, v1, 1.0)\n",
    "\n",
    "res[\"full\"] = {\n",
    "    \"u\": [u1.detach().cpu()],\n",
    "    \"v\": [v1.detach().cpu()],\n",
    "    \"metrics\": [metrics_full],\n",
    "    \"loss\": [loss_full],\n",
    "    \"norm\": [norm_full],\n",
    "    \"true_loss\": [true_loss_full],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5439e",
   "metadata": {},
   "source": [
    "## Check if it is ETF (It works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_proj, v1_proj = plot_embeddings(u1[:30], v1[:30], d)\n",
    "# first see if u = v\n",
    "print(\"||u-v|| = {}\".format(torch.norm(u1-v1)))\n",
    "# now see if the inner products are equal\n",
    "print(\"u^T v={}\".format(u1 @ v1.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ddf0e0",
   "metadata": {},
   "source": [
    "## Now minimize mini-batch loss over all specific batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_utils import create_inverse_greedy_batches_with_K, create_balance_greedy_batches, create_greedy_batches\n",
    "def get_random_batch_idxs(N, B=2):\n",
    "    batch_idxs = np.arange(N)\n",
    "    np.random.shuffle(batch_idxs)\n",
    "    return batch_idxs.reshape(-1, B).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56572915",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_selections = ['full', 'f', 's', 'g', 'bg', 'ig'] #['full', 'f', 's', 'g', 'bg', 'ig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch_selection in enumerate(batch_selections):\n",
    "    if batch_selection == 'full':\n",
    "        continue\n",
    "\n",
    "    res[batch_selection] = copy.deepcopy(dict_template)\n",
    "\n",
    "    for j, B in enumerate(Bs):\n",
    "        #######\n",
    "        set_seed(42)\n",
    "        u2, v2 = get_embeddings(N, d)\n",
    "        if i==1 and j==0: # plot once only\n",
    "            _, _ = plot_embeddings(u2[:30], v2[:30], d)\n",
    "        param_list = [u2, v2]\n",
    "        #######\n",
    "        optimizer = torch.optim.SGD(param_list, lr=lr_full*(B/N) if lr_scaling else lr_full)\n",
    "\n",
    "        if batch_selection == 'f':\n",
    "            batch_idxs = get_random_batch_idxs(N, B)\n",
    "\n",
    "        loss_mini, norm_mini, true_loss_mini, metrics_mini = {}, {}, {}, {}\n",
    "        for step in tqdm(range(NUM_STEPS), desc=f\"[batch_selection:'{batch_selection}' | B:{B}] \"):\n",
    "            optimizer.zero_grad()\n",
    "            if batch_selection == 'f':\n",
    "                pass\n",
    "            elif batch_selection == 's':\n",
    "                batch_idxs = get_random_batch_idxs(N, B)\n",
    "            elif batch_selection == 'g':\n",
    "                batch_idxs = create_greedy_batches(N, B, u2, v2, 1.0, device=device, D=d)\n",
    "            elif batch_selection == 'bg':\n",
    "                batch_idxs = create_balance_greedy_batches(N, B, u2, v2, 1.0, device=device, D=d)\n",
    "            elif batch_selection == 'ig':\n",
    "                batch_idxs = create_inverse_greedy_batches_with_K(N, B, u2, v2, 1.0, device=device, D=d)\n",
    "            else:\n",
    "                raise NotImplementedError(f'{batch_selection} is not available for batching')\n",
    "\n",
    "            if gradient_accumulation:\n",
    "                loss = mini_batch_loss(u2, v2, batch_idxs=batch_idxs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                total_loss = 0\n",
    "                for batch_idx in batch_idxs:\n",
    "                    u_batch = u2[list(batch_idx)]\n",
    "                    v_batch = v2[list(batch_idx)]\n",
    "                    loss = clip_batch_loss(u_batch, v_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss\n",
    "                loss = total_loss / len(batch_idxs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                u2.data = F.normalize(u2.data, p=2.0, dim = 1)\n",
    "                v2.data = F.normalize(v2.data, p=2.0, dim = 1)\n",
    "            if step %logging_step == 0 or step == NUM_STEPS-1:\n",
    "                # print(step, batch_idxs[0], loss.detach().item(), clip_batch_loss(u2, v2).detach().item(), u2, v2)\n",
    "                norm = torch.norm(u2.grad.data)\n",
    "                # print(\"Step={} | Loss={} | Grad Norm={}\".format(step, loss, norm))\n",
    "                loss_mini[step], norm_mini[step], true_loss_mini[step] = loss.detach().item(), norm.detach().item(), clip_batch_loss(u2, v2).detach().item()\n",
    "                metrics_mini[step] = get_metrics(u2, v2, 1.0)\n",
    "                torch.save(u2, f\"{output_dir}/u2_{batch_selection}_{step}.pt\")\n",
    "                torch.save(v2, f\"{output_dir}/u2_{batch_selection}_{step}.pt\")\n",
    "\n",
    "        res[batch_selection][\"u\"] += [u2.detach().cpu()]\n",
    "        res[batch_selection][\"v\"] += [v2.detach().cpu()]\n",
    "        res[batch_selection][\"metrics\"] += [metrics_mini]\n",
    "        res[batch_selection][\"loss\"] += [loss_mini]\n",
    "        res[batch_selection][\"norm\"] += [norm_mini]\n",
    "        res[batch_selection][\"true_loss\"] += [true_loss_mini]\n",
    "    _, _ = plot_embeddings(u2[:30], v2[:30], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391b50d",
   "metadata": {},
   "source": [
    "## Define variables for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fe239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants and other global variables\n",
    "label_size = 25\n",
    "tick_size = 25\n",
    "title_size = 25\n",
    "subtitle_size = 28\n",
    "caption_size = 24\n",
    "line_width = 5\n",
    "legend_size = 20\n",
    "markersize=18\n",
    "#\n",
    "colors = ['#073B4C', '#FFD166', '#EF476F', '#06D6A0',  '#118AB2', '#4D4C7D', '#FFDAC0', '#F54952']\n",
    "colors2 = ['#dd3497', '#ae017e', '#7a0177', '#49006a'] # red-purple\n",
    "warm_colors= ['#C2E4FF', '#2a2a72', '#009ffd', '#ffa400']\n",
    "cold_colors = ['#DBDFFD', '#9BA3EB', '#646FD4', '#242F9B']\n",
    "#colors2 = ['#dd3497', '#49006A', '#AE017E', '#7A0177'] # red-purple\n",
    "# WT: '#073B4C'\n",
    "# IMP: '#FFD166'\n",
    "# EP: '#06D6A0'\n",
    "# SR: '#118AB2'  \n",
    "# GM: '#dd3497'\n",
    "# GM (reinit): '#ae017e'\n",
    "# GM (shuffle): '#7a0177'\n",
    "# GM (invert): '#49006a'\n",
    "\n",
    "fig_width = 8\n",
    "fig_height = 4\n",
    "x_label = [50, 20, 5, 2.5, 1.4, 0.5]\n",
    "x_lim = [0.48, 55.0]\n",
    "y_label_af = [i*10 for i in range(5,10)]\n",
    "y_lim_af = [48, 95]\n",
    "y_label_bf = [i*20-10 for i in range(1,6)]\n",
    "y_lim_bf = [8, 95]\n",
    "\n",
    "plot_eps = False #True #  set y-axis as (1-eps) instead of actual accuracy value\n",
    "compare_iclr22 = False # check in visualize-results (fix ticks issue)-Copy1.ipynb\n",
    "SPARSITY_FLIP=True # set sparsity = \"density\" for neurips camera ready\n",
    "KEEP_Y_LABEL=False # keep this on for only the first column of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "res_data = {}\n",
    "for batch_selection in batch_selections:\n",
    "    mul_coef = len(Bs) if batch_selection == 'full' else 1\n",
    "    res_data.update({\n",
    "        f'{batch_selection}_batch': Bs,\n",
    "        f'{batch_selection}_true_loss': [r[NUM_STEPS-1] for r in res[batch_selection][\"true_loss\"]] * mul_coef,\n",
    "        f'{batch_selection}_loss': [r[NUM_STEPS-1] for r in res[batch_selection][\"loss\"]] * mul_coef,\n",
    "        f'{batch_selection}_utov_R@1': [r[NUM_STEPS-1][\"image_to_text_R@1\"] for r in res[batch_selection][\"metrics\"]] * mul_coef,\n",
    "        f'{batch_selection}_vtou_R@1': [r[NUM_STEPS-1][\"text_to_image_R@1\"] for r in res[batch_selection][\"metrics\"]] * mul_coef,\n",
    "    })\n",
    "res_data_df = pd.DataFrame(data=res_data)\n",
    "res_data_df.to_csv(f'{output_dir}/res_data.csv', sep='|', encoding='utf-8-sig', index=False)\n",
    "res_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f500672",
   "metadata": {},
   "source": [
    "## Draw loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### True loss Plot ###\n",
    "\n",
    "labels = batch_selections\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n",
    "line_width=2\n",
    "markersize=10\n",
    "\n",
    "for i, batch_selection in enumerate(labels):\n",
    "    res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_true_loss', marker='s', style=\"--\", markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_true_loss\", ax=ax1)\n",
    "    # res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_loss', marker='^', markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_loss\", ax=ax1)\n",
    "    # plt.axhline(100, linestyle='--')\n",
    "\n",
    "## title, axis label\n",
    "# plt.title(\"{} train T->I, RN50\".format(measure), fontsize=title_size)\n",
    "ax1.tick_params(axis='both', labelsize=tick_size)\n",
    "ax1.set_xlabel(\"Batch Size\", fontsize=label_size)\n",
    "\n",
    "\n",
    "\n",
    "# ## tick, lim\n",
    "# plt.tick_params(labelsize=tick_size)\n",
    "# xticks_label = [1, 50, 100, 150, 200, 300]\n",
    "# # zoom1\n",
    "# # xticks_label = [0, 1, 5, 15]\n",
    "\n",
    "# plt.xticks(xticks_label, xticks_label)\n",
    "\n",
    "# yticks_label = [0.0, 2.0, 3.0, 4.0, 5.0]\n",
    "yticks_label = [4.0, 4.1, 4.2]\n",
    "plt.yticks(yticks_label, yticks_label)\n",
    "\n",
    "# x_lim = [-1, 160]\n",
    "# # zoom1\n",
    "# # x_lim = [-1, 5]\n",
    "# y_lim = [10, 100]\n",
    "\n",
    "# ax1.set_xlim(x_lim)\n",
    "# ax1.set_ylim(y_lim)\n",
    "\n",
    "\n",
    "#plt.yticks(yticks_label, yticks_label)\n",
    "#ax1.set_ylim(y_lim_bf)\n",
    "ax1.set_ylabel(\" \", fontsize=label_size)\n",
    "# ax1.set_ylabel(\"Top-1 Recall (%)\", fontsize=label_size)\n",
    "\n",
    "\n",
    "    \n",
    "## Legend\n",
    "handles_ord, labels_ord = plt.gca().get_legend_handles_labels()\n",
    "last_idx = len(labels)\n",
    "#last_idx = len(labels) - 1\n",
    "order = [i for i in range(last_idx)]\n",
    "#order.insert(0,last_idx)\n",
    "\n",
    "ax1.legend([handles_ord[idx] for idx in order],[labels_ord[idx] for idx in order], handlelength=2,\n",
    "           fontsize=legend_size, loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "# ax1.get_legend().remove()\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.savefig(f'{output_dir}/true_loss_all.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd372f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss Plot ###\n",
    "\n",
    "labels = batch_selections\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n",
    "line_width=2\n",
    "markersize=10\n",
    "\n",
    "for i, batch_selection in enumerate(labels):\n",
    "    # res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_true_loss', marker='s', style=\"--\", markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_true_loss\", ax=ax1)\n",
    "    res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_loss', marker='^', markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_loss\", ax=ax1)\n",
    "    # plt.axhline(100, linestyle='--')\n",
    "\n",
    "## title, axis label\n",
    "# plt.title(\"{} train T->I, RN50\".format(measure), fontsize=title_size)\n",
    "ax1.tick_params(axis='both', labelsize=tick_size)\n",
    "ax1.set_xlabel(\"Batch Size\", fontsize=label_size)\n",
    "\n",
    "\n",
    "\n",
    "# ## tick, lim\n",
    "# plt.tick_params(labelsize=tick_size)\n",
    "# xticks_label = [1, 50, 100, 150, 200, 300]\n",
    "# # zoom1\n",
    "# # xticks_label = [0, 1, 5, 15]\n",
    "\n",
    "# plt.xticks(xticks_label, xticks_label)\n",
    "\n",
    "yticks_label = [0.0, 2.0, 3.0, 4.0, 5.0]\n",
    "# yticks_label = [4.0, 4.1]\n",
    "plt.yticks(yticks_label, yticks_label)\n",
    "\n",
    "# x_lim = [-1, 160]\n",
    "# # zoom1\n",
    "# # x_lim = [-1, 5]\n",
    "# y_lim = [10, 100]\n",
    "\n",
    "# ax1.set_xlim(x_lim)\n",
    "# ax1.set_ylim(y_lim)\n",
    "\n",
    "\n",
    "#plt.yticks(yticks_label, yticks_label)\n",
    "#ax1.set_ylim(y_lim_bf)\n",
    "ax1.set_ylabel(\" \", fontsize=label_size)\n",
    "# ax1.set_ylabel(\"Top-1 Recall (%)\", fontsize=label_size)\n",
    "\n",
    "\n",
    "    \n",
    "## Legend\n",
    "handles_ord, labels_ord = plt.gca().get_legend_handles_labels()\n",
    "last_idx = len(labels)\n",
    "#last_idx = len(labels) - 1\n",
    "order = [i for i in range(last_idx)]\n",
    "#order.insert(0,last_idx)\n",
    "\n",
    "ax1.legend([handles_ord[idx] for idx in order],[labels_ord[idx] for idx in order], handlelength=2,\n",
    "           fontsize=legend_size, loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "# ax1.get_legend().remove()\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.savefig(f'{output_dir}/loss_all.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5345d",
   "metadata": {},
   "source": [
    "## Draw R@1 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab43758",
   "metadata": {},
   "outputs": [],
   "source": [
    "### R@1 Plot ###\n",
    "\n",
    "labels = batch_selections\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n",
    "line_width=2\n",
    "markersize=10\n",
    "\n",
    "for i, batch_selection in enumerate(labels):\n",
    "    res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_utov_R@1', marker='s', style=\"--\", markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_utov_R@1\", ax=ax1)\n",
    "    res_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_vtou_R@1', marker='^', markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_vtou_R@1\", ax=ax1)\n",
    "    # plt.axhline(100, linestyle='--')\n",
    "\n",
    "## title, axis label\n",
    "# plt.title(\"{} train T->I, RN50\".format(measure), fontsize=title_size)\n",
    "ax1.tick_params(axis='both', labelsize=tick_size)\n",
    "ax1.set_xlabel(\"Batch Size\", fontsize=label_size)\n",
    "\n",
    "\n",
    "\n",
    "# ## tick, lim\n",
    "# plt.tick_params(labelsize=tick_size)\n",
    "# xticks_label = [1, 50, 100, 150, 200, 300]\n",
    "# # zoom1\n",
    "# # xticks_label = [0, 1, 5, 15]\n",
    "\n",
    "# plt.xticks(xticks_label, xticks_label)\n",
    "\n",
    "yticks_label = [0.0, 1.0]\n",
    "plt.yticks(yticks_label, yticks_label)\n",
    "\n",
    "# x_lim = [-1, 160]\n",
    "# # zoom1\n",
    "# # x_lim = [-1, 5]\n",
    "# y_lim = [10, 100]\n",
    "\n",
    "# ax1.set_xlim(x_lim)\n",
    "# ax1.set_ylim(y_lim)\n",
    "\n",
    "\n",
    "#plt.yticks(yticks_label, yticks_label)\n",
    "#ax1.set_ylim(y_lim_bf)\n",
    "ax1.set_ylabel(\" \", fontsize=label_size)\n",
    "# ax1.set_ylabel(\"Top-1 Recall (%)\", fontsize=label_size)\n",
    "\n",
    "\n",
    "    \n",
    "## Legend\n",
    "handles_ord, labels_ord = plt.gca().get_legend_handles_labels()\n",
    "last_idx = len(labels) * 2\n",
    "#last_idx = len(labels) - 1\n",
    "order = [i for i in range(last_idx)]\n",
    "#order.insert(0,last_idx)\n",
    "\n",
    "ax1.legend([handles_ord[idx] for idx in order],[labels_ord[idx] for idx in order], handlelength=2,\n",
    "           fontsize=legend_size, loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "# ax1.get_legend().remove()\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.savefig(f'{output_dir}/R@1_all.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dca7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecb17f",
   "metadata": {},
   "source": [
    "## Draw logging plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f87d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "logging_data = {}\n",
    "for batch_selection in batch_selections:\n",
    "    logging_data.update({\n",
    "        f'{batch_selection}_step': res[batch_selection][\"true_loss\"][0].keys(),\n",
    "    })\n",
    "    for B_i, B in enumerate(Bs):\n",
    "        if batch_selection == \"full\":\n",
    "            B_i = 0\n",
    "        logging_data.update({\n",
    "            f'{batch_selection}_{B}_true_loss': res[batch_selection][\"true_loss\"][B_i].values(),\n",
    "            f'{batch_selection}_{B}_loss': res[batch_selection][\"loss\"][B_i].values(),\n",
    "            f'{batch_selection}_{B}_utov_R@1': [l[\"image_to_text_R@1\"] for l in res[batch_selection][\"metrics\"][B_i].values()],\n",
    "            f'{batch_selection}_{B}_vtou_R@1': [l[\"text_to_image_R@1\"] for l in res[batch_selection][\"metrics\"][B_i].values()],\n",
    "        })\n",
    "logging_data_df = pd.DataFrame(data=logging_data)\n",
    "logging_data_df.to_csv(f'{output_dir}/logging_data.csv', sep='|', encoding='utf-8-sig', index=False)\n",
    "logging_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logging Plot ###\n",
    "\n",
    "labels = batch_selections\n",
    "\n",
    "for i, batch_selection in enumerate(labels):\n",
    "\n",
    "    for measure in [\"true_loss\", \"loss\", \"utov_R@1\", \"vtou_R@1\"]:\n",
    "        fig, ax1 = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        line_width=2\n",
    "        markersize=10\n",
    "\n",
    "        for B_i, B in enumerate(Bs):\n",
    "            logging_data_df.plot(x=f'{batch_selection}_step', y=f'{batch_selection}_{B}_{measure}', marker='s', style=\"--\", markersize=markersize, linewidth=line_width, c=colors[i+B_i], label=f\"{batch_selection}_{measure}_{B}\", ax=ax1)\n",
    "        # logging_data_df.plot(x=f'{batch_selection}_batch', y=f'{batch_selection}_loss', marker='^', markersize=markersize, linewidth=line_width, c=colors[i], label=labels[i]+\"_loss\", ax=ax1)\n",
    "        # plt.axhline(100, linestyle='--')\n",
    "\n",
    "        ## title, axis label\n",
    "        # plt.title(\"{} train T->I, RN50\".format(measure), fontsize=title_size)\n",
    "        ax1.tick_params(axis='both', labelsize=tick_size)\n",
    "        ax1.set_xlabel(\"step\", fontsize=label_size)\n",
    "\n",
    "\n",
    "\n",
    "        # ## tick, lim\n",
    "        # plt.tick_params(labelsize=tick_size)\n",
    "        # xticks_label = [1, 50, 100, 150, 200, 300]\n",
    "        # # zoom1\n",
    "        # # xticks_label = [0, 1, 5, 15]\n",
    "\n",
    "        # plt.xticks(xticks_label, xticks_label)\n",
    "\n",
    "        if \"R@1\" in measure:\n",
    "            yticks_label = [0.0, 1.0]\n",
    "        else:\n",
    "            yticks_label = [0.0, 2.0, 3.0, 4.0, 5.0]\n",
    "        plt.yticks(yticks_label, yticks_label)\n",
    "\n",
    "        # x_lim = [-1, 160]\n",
    "        # # zoom1\n",
    "        # # x_lim = [-1, 5]\n",
    "        # y_lim = [10, 100]\n",
    "\n",
    "        # ax1.set_xlim(x_lim)\n",
    "        # ax1.set_ylim(y_lim)\n",
    "\n",
    "\n",
    "        #plt.yticks(yticks_label, yticks_label)\n",
    "        #ax1.set_ylim(y_lim_bf)\n",
    "        ax1.set_ylabel(\" \", fontsize=label_size)\n",
    "        # ax1.set_ylabel(\"Top-1 Recall (%)\", fontsize=label_size)\n",
    "\n",
    "\n",
    "                \n",
    "        ## Legend\n",
    "        handles_ord, labels_ord = plt.gca().get_legend_handles_labels()\n",
    "        last_idx = len(Bs)\n",
    "        #last_idx = len(labels) - 1\n",
    "        order = [i for i in range(last_idx)]\n",
    "        #order.insert(0,last_idx)\n",
    "\n",
    "        if batch_selection == \"full\":\n",
    "            ax1.get_legend().remove()\n",
    "        else:\n",
    "            ax1.legend([handles_ord[idx] for idx in order],[labels_ord[idx] for idx in order], handlelength=2,\n",
    "                    fontsize=legend_size, loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "        ax1.grid(True)\n",
    "\n",
    "        plt.savefig(f'{output_dir}/logging_{batch_selection}_{measure}.png', format='png', dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a16bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
